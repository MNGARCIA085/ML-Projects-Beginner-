{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 441 files belonging to 4 classes.\n",
      "Using 353 files for training.\n",
      "Using 88 files for validation.\n"
     ]
    }
   ],
   "source": [
    "image_size = (256, 256)\n",
    "batch_size = 128\n",
    "\n",
    "train_ds, val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    \"archive\\Sports-celebrity images\",\n",
    "    validation_split=0.2,\n",
    "    subset=\"both\",\n",
    "    shuffle=True,\n",
    "    seed=1337,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kane Williamson', 'Kobe Bryant', 'Maria Sharapova', 'Ronaldo']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=train_ds.class_names #Classes of data\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agregar prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply `data_augmentation` to the training images.\n",
    "train_ds = train_ds.map(\n",
    "    lambda img, label: (data_augmentation(img), label),\n",
    "    num_parallel_calls=tf.data.AUTOTUNE,\n",
    ")\n",
    "# Prefetching samples in GPU memory helps maximize GPU utilization.\n",
    "#train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "#val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds=train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds=val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#test_data=test_data.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels=3\n",
    "input_shape=(image_size,image_size,channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    #x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    # dropout, sin batch norm\n",
    "    \n",
    "    \n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    \n",
    "    \n",
    "    x = layers.Dropout(0.4)(x) # nuevo\n",
    "    \n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.AveragePooling2D(3, strides=2, padding=\"same\")(x) # Average ok\n",
    "    \n",
    "    \n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    \n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(64,activation='relu')(x)\n",
    "    \n",
    "    #\n",
    "    x = layers.Dropout(0.4)(x) # sin esto no anda mal pero el modelo tiene overfitting\n",
    "    \n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=4)\n",
    "#model = make_model(input_shape=input_shape, num_classes=4)\n",
    "#keras.utils.plot_model(model, show_shapes=True)\n",
    "\n",
    "\n",
    "#model = make_model(input_shape=image_size + (3,), num_classes=4)\n",
    "#model = make_model(input_shape=input_shape, num_classes=4)\n",
    "#keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/66036271/splitting-a-tensorflow-dataset-into-training-test-and-validation-sets-from-ker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 24s 6s/step - loss: 1.6241 - accuracy: 0.2805 - val_loss: 2.5373 - val_accuracy: 0.2955\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.4090 - accuracy: 0.3541 - val_loss: 2.4955 - val_accuracy: 0.3409\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.3322 - accuracy: 0.3654 - val_loss: 1.5283 - val_accuracy: 0.4091\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.2708 - accuracy: 0.4419 - val_loss: 1.4548 - val_accuracy: 0.4545\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.2362 - accuracy: 0.4221 - val_loss: 1.6642 - val_accuracy: 0.3864\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.1851 - accuracy: 0.4561 - val_loss: 2.0005 - val_accuracy: 0.4205\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.1709 - accuracy: 0.4646 - val_loss: 2.2499 - val_accuracy: 0.4432\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.1197 - accuracy: 0.5156 - val_loss: 2.3492 - val_accuracy: 0.3750\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.0852 - accuracy: 0.5212 - val_loss: 2.4502 - val_accuracy: 0.3864\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.0647 - accuracy: 0.5326 - val_loss: 2.5676 - val_accuracy: 0.3977\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.0981 - accuracy: 0.5411 - val_loss: 3.0489 - val_accuracy: 0.4205\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.0545 - accuracy: 0.5807 - val_loss: 3.1340 - val_accuracy: 0.4773\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 1.0161 - accuracy: 0.5751 - val_loss: 2.9449 - val_accuracy: 0.4886\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.9884 - accuracy: 0.5666 - val_loss: 3.1731 - val_accuracy: 0.4318\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 16s 5s/step - loss: 1.0079 - accuracy: 0.5694 - val_loss: 3.7760 - val_accuracy: 0.3750\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.9734 - accuracy: 0.5694 - val_loss: 3.6694 - val_accuracy: 0.3977\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.9658 - accuracy: 0.6006 - val_loss: 2.4644 - val_accuracy: 0.4773\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.9378 - accuracy: 0.5949 - val_loss: 1.5902 - val_accuracy: 0.5795\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.9074 - accuracy: 0.6289 - val_loss: 1.4880 - val_accuracy: 0.5795\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.8763 - accuracy: 0.6346 - val_loss: 1.8975 - val_accuracy: 0.5455\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.8518 - accuracy: 0.6487 - val_loss: 2.3268 - val_accuracy: 0.5227\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.8344 - accuracy: 0.6657 - val_loss: 2.5777 - val_accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.8600 - accuracy: 0.6232 - val_loss: 2.7752 - val_accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.8520 - accuracy: 0.6346 - val_loss: 3.3188 - val_accuracy: 0.3977\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import callbacks\n",
    "\n",
    "model.compile(optimizer=\"adam\",#keras.optimizers.Adam(1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history=model.fit(train_ds, epochs=50, verbose=1,\n",
    "                  batch_size=batch_size,validation_data=val_ds,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model(classes):\n",
    "    model=models.Sequential([layers.Conv2D(32,(3,3),activation=\"relu\",\n",
    "                                       input_shape=input_shape),\n",
    "                                       layers.MaxPooling2D((2,2)),\n",
    "                         \n",
    "                         layers.Conv2D(64,(3,3),activation=\"relu\"),\n",
    "                                       layers.MaxPooling2D((2,2)),\n",
    "                         \n",
    "                         layers.Dropout(.5),\n",
    "                         \n",
    "                         layers.Conv2D(64,(3,3),activation=\"relu\"),\n",
    "                                       layers.MaxPooling2D((2,2)),\n",
    "                         \n",
    "                         layers.Conv2D(64,(3,3),activation=\"relu\"),\n",
    "                                       layers.MaxPooling2D((2,2)),\n",
    "                         \n",
    "                         layers.Dropout(.5),\n",
    "                         \n",
    "                         layers.Conv2D(64,(3,3),activation=\"relu\"),\n",
    "                                       layers.MaxPooling2D((2,2)),\n",
    "                         \n",
    "                         layers.Conv2D(64,(3,3),activation=\"relu\"),\n",
    "                                       layers.MaxPooling2D((2,2)),\n",
    "                         \n",
    "                         layers.Flatten(),\n",
    "                         layers.Dense(64,activation=\"relu\"),\n",
    "                         layers.Dense(len(classes),activation=\"softmax\")   \n",
    "                        ])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = model(classes)\n",
    "model.build(input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 53s 12s/step - loss: 2.8129 - accuracy: 0.2776 - val_loss: 5.2526 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 44s 10s/step - loss: 1.7640 - accuracy: 0.3059 - val_loss: 2.2971 - val_accuracy: 0.3636\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 42s 10s/step - loss: 1.3921 - accuracy: 0.3966 - val_loss: 2.8175 - val_accuracy: 0.4432\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 42s 10s/step - loss: 1.3619 - accuracy: 0.4108 - val_loss: 3.7731 - val_accuracy: 0.3182\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 43s 10s/step - loss: 1.2146 - accuracy: 0.4958 - val_loss: 3.9558 - val_accuracy: 0.3750\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 24s 4s/step - loss: 1.2467 - accuracy: 0.4759 - val_loss: 3.8387 - val_accuracy: 0.4659\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 16s 4s/step - loss: 1.1988 - accuracy: 0.4646 - val_loss: 3.9586 - val_accuracy: 0.3977\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 16s 4s/step - loss: 1.1062 - accuracy: 0.5241 - val_loss: 4.0863 - val_accuracy: 0.3409\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 1.0467 - accuracy: 0.5411 - val_loss: 4.2816 - val_accuracy: 0.3409\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 1.0061 - accuracy: 0.5779 - val_loss: 3.7025 - val_accuracy: 0.3750\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 1.0325 - accuracy: 0.5836 - val_loss: 2.8405 - val_accuracy: 0.3864\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 0.9863 - accuracy: 0.6119 - val_loss: 2.6193 - val_accuracy: 0.3864\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 0.9661 - accuracy: 0.6261 - val_loss: 2.5490 - val_accuracy: 0.4091\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 0.9514 - accuracy: 0.6176 - val_loss: 2.3250 - val_accuracy: 0.4091\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 0.8676 - accuracy: 0.6459 - val_loss: 2.3137 - val_accuracy: 0.4091\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 0.8461 - accuracy: 0.6714 - val_loss: 2.5069 - val_accuracy: 0.3750\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 18s 5s/step - loss: 0.8570 - accuracy: 0.6601 - val_loss: 2.7140 - val_accuracy: 0.3636\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 19s 5s/step - loss: 0.8597 - accuracy: 0.6629 - val_loss: 2.7412 - val_accuracy: 0.3750\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 22s 5s/step - loss: 0.7813 - accuracy: 0.6742 - val_loss: 2.8102 - val_accuracy: 0.3409\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 21s 5s/step - loss: 0.8119 - accuracy: 0.6601 - val_loss: 2.7461 - val_accuracy: 0.3636\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 21s 5s/step - loss: 0.7936 - accuracy: 0.6601 - val_loss: 2.5580 - val_accuracy: 0.3977\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 20s 5s/step - loss: 0.7531 - accuracy: 0.7195 - val_loss: 2.4720 - val_accuracy: 0.3750\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "history=model.fit(train_ds, epochs=50, verbose=1,\n",
    "                  batch_size=batch_size,validation_data=val_ds,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 15.7612 - accuracy: 0.2614\n"
     ]
    }
   ],
   "source": [
    "scores=model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_shape, num_classes):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Entry block\n",
    "    #x = layers.Rescaling(1.0 / 255)(inputs)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    #x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    #x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "    #x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x) # Average\n",
    "    \n",
    "    \n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if num_classes == 2:\n",
    "        activation = \"sigmoid\"\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = \"softmax\"\n",
    "        units = num_classes\n",
    "\n",
    "    #x = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(64,activation='relu')(x)\n",
    "    \n",
    "    outputs = layers.Dense(units, activation=activation)(x)\n",
    "    \n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "\n",
    "model = make_model(input_shape=image_size + (3,), num_classes=4)\n",
    "#keras.utils.plot_model(model, show_shapes=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
